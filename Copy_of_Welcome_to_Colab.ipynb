{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KaiTheWizard/ParaPhraseDetectProject/blob/main/Copy_of_Welcome_to_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czxnmyB8LCXC",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Installing and Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cBuAS05XLIer"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (4.51.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers scikit-learn torch pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kt0WZM09MGZN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx0IfqU7KPQD",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1OsWTcTzK4zL"
   },
   "outputs": [],
   "source": [
    "# Load QQP dataset\n",
    "dataset = load_dataset(\"glue\", \"qqp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5gsNS8mcK5IT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (363846, 4), 'validation': (40430, 4), 'test': (390965, 4)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gXNV5IMNQc_"
   },
   "source": [
    "## Displaying the first and last 5 samples of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KW3x0So8M2x1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  How is the life of a math student? Could you d...   \n",
       "1                How do I control my horny emotions?   \n",
       "2       What causes stool color to change to yellow?   \n",
       "3                        What can one do after MBBS?   \n",
       "4  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                           question2  label  idx  \n",
       "0  Which level of prepration is enough for the ex...      0    0  \n",
       "1                 How do you control your horniness?      1    1  \n",
       "2  What can cause stool to come out as little balls?      0    2  \n",
       "3                       What do i do after my MBBS ?      1    3  \n",
       "4  Would a second airport in Sydney, Australia be...      0    4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rSh3UfNTM203"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>363841</th>\n",
       "      <td>How do I make money flying my drone?</td>\n",
       "      <td>How can I use a dji phantom to make money</td>\n",
       "      <td>1</td>\n",
       "      <td>363841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363842</th>\n",
       "      <td>What can you do with an economics degree?</td>\n",
       "      <td>What jobs can you get with an economics degree?</td>\n",
       "      <td>1</td>\n",
       "      <td>363842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363843</th>\n",
       "      <td>What type of current does a battery produce?</td>\n",
       "      <td>How does a generator work and produce current?</td>\n",
       "      <td>0</td>\n",
       "      <td>363843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363844</th>\n",
       "      <td>Grammar: What is difference between schedule a...</td>\n",
       "      <td>How do I understand the difference between the...</td>\n",
       "      <td>0</td>\n",
       "      <td>363844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363845</th>\n",
       "      <td>What is the easiest way to earn money using in...</td>\n",
       "      <td>How can I earn money online easily?</td>\n",
       "      <td>1</td>\n",
       "      <td>363845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1  \\\n",
       "363841               How do I make money flying my drone?   \n",
       "363842          What can you do with an economics degree?   \n",
       "363843       What type of current does a battery produce?   \n",
       "363844  Grammar: What is difference between schedule a...   \n",
       "363845  What is the easiest way to earn money using in...   \n",
       "\n",
       "                                                question2  label     idx  \n",
       "363841          How can I use a dji phantom to make money      1  363841  \n",
       "363842    What jobs can you get with an economics degree?      1  363842  \n",
       "363843     How does a generator work and produce current?      0  363843  \n",
       "363844  How do I understand the difference between the...      0  363844  \n",
       "363845                How can I earn money online easily?      1  363845  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].to_pandas().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJ40TBXRSExE"
   },
   "source": [
    "## Checking for missing, null and duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vT2fk5AnOCvA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 363846 entries, 0 to 363845\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   question1  363846 non-null  object\n",
      " 1   question2  363846 non-null  object\n",
      " 2   label      363846 non-null  int64 \n",
      " 3   idx        363846 non-null  int32 \n",
      "dtypes: int32(1), int64(1), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset['train'].to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "955E21-1OKfx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question1    0\n",
       "question2    0\n",
       "label        0\n",
       "idx          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].to_pandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yokVd5A8OODI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].to_pandas().duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkAdlotmDYsS"
   },
   "source": [
    "## Counting Total number of unique words in the QQP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51T_MkqE_5Xe",
    "outputId": "ad2aa6b0-953a-4dc0-f496-b054d5e58cd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 27010\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer  # importing tokenizer\n",
    "\n",
    "# Load tokenizer and dataset\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Collect unique tokens from all splits\n",
    "unique_tokens = set()\n",
    "splits = ['train', 'validation', 'test']\n",
    "\n",
    "for split in splits:\n",
    "    for example in dataset[split]:\n",
    "        if example.get('question1'):\n",
    "            unique_tokens.update(tokenizer.tokenize(example['question1']))\n",
    "        if example.get('question2'):\n",
    "            unique_tokens.update(tokenizer.tokenize(example['question2']))\n",
    "\n",
    "# Save unique tokens to a file\n",
    "with open(\"unique_tokens.txt\", \"w\") as file:\n",
    "    for token in sorted(unique_tokens):\n",
    "        file.write(f\"{token}\\n\")\n",
    "\n",
    "print(f\"Total unique tokens: {len(unique_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hluf0PzxQbxj"
   },
   "source": [
    "## Calculating the optimal no. of embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1OBponZ1QYea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 27010\n",
      "Optimal number of dimensions per token: 50\n"
     ]
    }
   ],
   "source": [
    "# Calculate optimal embedding dimensions\n",
    "nw = len(unique_tokens)\n",
    "dimensions = min(10 * np.ceil(np.log10(nw)), 300)\n",
    "print(f\"Total unique tokens: {nw}\")\n",
    "print(f\"Optimal number of dimensions per token: {int(dimensions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naxjlyxPNpHe"
   },
   "source": [
    "## Spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "06N98CKMM2sV"
   },
   "outputs": [],
   "source": [
    "train_df = dataset['train'].to_pandas()\n",
    "validation_df = dataset['validation'].to_pandas()\n",
    "test_df = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KvLXZ7BhM2vW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data is : (363846, 4)\n",
      "Shape of testing data is : (390965, 4)\n",
      "Shape of validation data is : (40430, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of training data is : {train_df.shape}\")\n",
    "print(f\"Shape of testing data is : {test_df.shape}\")\n",
    "print(f\"Shape of validation data is : {validation_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_rmn7aQKjnC"
   },
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "AvXbsMGRMR4t"
   },
   "outputs": [],
   "source": [
    "# Models for embeddings\n",
    "models = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UD9GzBjGp20f"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_embeddings(text, model_name, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embedding = outputs.last_hidden_state[:, 0, :]  # Extract CLS token embedding\n",
    "\n",
    "    # Project to 50 dimensions if needed\n",
    "    if embedding.shape[1] != 50:\n",
    "        embedding = F.adaptive_avg_pool1d(embedding.unsqueeze(0), 50).squeeze(0)\n",
    "\n",
    "    return embedding.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing embeddings for model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  - Processing train split\n",
      "  - Processing test split\n",
      "  - Processing validation split\n",
      "\n",
      "Processing embeddings for model: sentence-transformers/paraphrase-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1749152e61403ab46fe93297331c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c086b76a78c04719b95bced39818845c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6944fcdd9c934881b917925b81b58ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d690b2f561b4236a2bdadedff06e3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e440281e3541a699d3594acc99b79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c140f7b553964c4da444fe543894213f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing train split\n",
      "  - Processing test split\n",
      "  - Processing validation split\n",
      "\n",
      "Processing embeddings for model: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee73e1b8ed144ea6af818f20cd4448be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd685c88b29f4537b3d2bc4c20ae846d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e99f21517dd4126855b444c8e01a66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e494dac98b4bcf8f12fdfb9cf67a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98aaef3854d9428bab1c6ca7986d92e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing train split\n",
      "  - Processing test split\n",
      "  - Processing validation split\n",
      "\n",
      "Processing embeddings for model: roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c76d8f2d1a40acaca60fb109b6a948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a0b2893e9445d38ec089cc397dd397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea27126e3f240eabddc99363d9efa21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d6fea95df943d2ade8376fdac7f887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0045beb66424c29b7e7cf2ad0a33865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015bfc36d61645b983986c2c19dd7ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing train split\n",
      "  - Processing test split\n",
      "  - Processing validation split\n",
      "\n",
      "Processing embeddings for model: microsoft/deberta-v3-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c00aa7e8c04a7ca173d2832112ae51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d16a3364464513a10a042ff41c2c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4f1cf011c94b4dadcb09c0d3112d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1587\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[0;34m(self, tiktoken_url)\u001b[0m\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1587\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtiktoken\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_tiktoken_bpe\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1730\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1726\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1624\u001b[0m, in \u001b[0;36mTikTokenConverter.converted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[0;32m-> 1624\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1625\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mSequence(\n\u001b[1;32m   1626\u001b[0m         [\n\u001b[1;32m   1627\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mSplit(Regex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern), behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misolated\u001b[39m\u001b[38;5;124m\"\u001b[39m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1628\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space, use_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1629\u001b[0m         ]\n\u001b[1;32m   1630\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1617\u001b[0m, in \u001b[0;36mTikTokenConverter.tokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1617\u001b[0m     vocab_scores, merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1618\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(BPE(vocab_scores, merges, fuse_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1589\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[0;34m(self, tiktoken_url)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1589\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1590\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1591\u001b[0m     )\n\u001b[1;32m   1593\u001b[0m bpe_ranks \u001b[38;5;241m=\u001b[39m load_tiktoken_bpe(tiktoken_url)\n",
      "\u001b[0;31mValueError\u001b[0m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing embeddings for model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1027\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2062\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2059\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2060\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2073\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2302\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2302\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2304\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2307\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py:103\u001b[0m, in \u001b[0;36mDebertaV2TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_lower_case\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_lower_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_by_punct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_by_punct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_by_punct \u001b[38;5;241m=\u001b[39m split_by_punct\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:139\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m--> 139\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1732\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[1;32m   1728\u001b[0m         vocab_file\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file,\n\u001b[1;32m   1729\u001b[0m         additional_special_tokens\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39madditional_special_tokens,\n\u001b[1;32m   1730\u001b[0m     )\u001b[38;5;241m.\u001b[39mconverted()\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1733\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1734\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1735\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1736\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize embedding data dictionary for all splits\n",
    "embedding_data = {\n",
    "    \"train\": {},\n",
    "    \"test\": {},\n",
    "    \"validation\": {}\n",
    "}\n",
    "\n",
    "# Loop through models\n",
    "for model_name in models:\n",
    "    print(f\"\\nProcessing embeddings for model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        if split not in dataset:\n",
    "            print(f\"Skipping {split} split as it's not in the dataset.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  - Processing {split} split\")\n",
    "\n",
    "        q1_list = dataset[split]['question1']\n",
    "        q2_list = dataset[split]['question2']\n",
    "\n",
    "        # Randomly sample 1000 indices (or less if not enough data)\n",
    "        sample_size = min(1000, len(q1_list))\n",
    "        indices = random.sample(range(len(q1_list)), sample_size)\n",
    "\n",
    "        sampled_q1 = [q1_list[i] for i in indices]\n",
    "        sampled_q2 = [q2_list[i] for i in indices]\n",
    "\n",
    "        # Generate embeddings for sampled questions\n",
    "        q1_embeddings = np.array([get_embeddings(q, model_name, tokenizer, model) for q in sampled_q1])\n",
    "        q2_embeddings = np.array([get_embeddings(q, model_name, tokenizer, model) for q in sampled_q2])\n",
    "\n",
    "        # Save to disk\n",
    "        model_name_safe = model_name.replace(\"/\", \"_\")\n",
    "        np.save(f\"{model_name_safe}_{split}_q1.npy\", q1_embeddings)\n",
    "        np.save(f\"{model_name_safe}_{split}_q2.npy\", q2_embeddings)\n",
    "\n",
    "        # Store in dict\n",
    "        embedding_data[split][model_name] = (q1_embeddings, q2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set same seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize label data dictionary\n",
    "label_data = {\n",
    "    \"train\": None,\n",
    "    \"test\": None,\n",
    "    \"validation\": None\n",
    "}\n",
    "\n",
    "# Function to sample labels\n",
    "def sample_labels_from_dataset(dataset, sample_size=1000):\n",
    "    sampled_labels_per_split = {}\n",
    "\n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        if split not in dataset:\n",
    "            print(f\"Skipping {split} split as it's not in the dataset.\")\n",
    "            continue\n",
    "\n",
    "        labels = dataset[split]['label']\n",
    "        sample_size_actual = min(sample_size, len(labels))\n",
    "\n",
    "        # Resample using same seed to get same indices\n",
    "        indices = random.sample(range(len(labels)), sample_size_actual)\n",
    "        sampled_labels = [labels[i] for i in indices]\n",
    "\n",
    "        sampled_labels_per_split[split] = sampled_labels\n",
    "\n",
    "    return sampled_labels_per_split\n",
    "\n",
    "# Run it and store\n",
    "label_data = sample_labels_from_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'sentence-transformers/all-MiniLM-L6-v2': (array([[-0.02449382,  0.00483341, -0.14260648, ..., -0.12394938,\n",
       "            0.1475454 , -0.01830261],\n",
       "          [-0.03890347, -0.01542168, -0.0395979 , ...,  0.05827474,\n",
       "           -0.01583578,  0.0274667 ],\n",
       "          [ 0.03526154,  0.23093592,  0.16593309, ..., -0.04491486,\n",
       "           -0.14556651, -0.07633272],\n",
       "          ...,\n",
       "          [ 0.06438376, -0.1007113 ,  0.13367853, ...,  0.00330999,\n",
       "            0.1301008 , -0.00107935],\n",
       "          [-0.07746263, -0.13325864, -0.01516933, ...,  0.00631686,\n",
       "           -0.13136286, -0.10768058],\n",
       "          [-0.00115653,  0.04987596,  0.15073262, ..., -0.036385  ,\n",
       "            0.04860617, -0.00865657]], dtype=float32),\n",
       "   array([[ 9.83612612e-04, -4.09793202e-03, -1.78865224e-01, ...,\n",
       "           -1.29865631e-01,  1.36774704e-01, -9.39817354e-03],\n",
       "          [ 3.87135595e-02, -5.84291667e-02, -1.63736448e-04, ...,\n",
       "            9.72618163e-02,  6.76678121e-02, -5.06729260e-03],\n",
       "          [-1.54153695e-02,  2.39958525e-01,  1.46496594e-01, ...,\n",
       "            4.16771229e-03, -1.40874729e-01, -2.60008946e-02],\n",
       "          ...,\n",
       "          [ 6.99137747e-02, -1.02933519e-01,  1.32801190e-01, ...,\n",
       "            2.03222428e-02,  1.04803115e-01, -3.34040821e-03],\n",
       "          [ 1.38774812e-02, -1.61422253e-01, -1.53179383e-02, ...,\n",
       "            1.44875897e-02, -5.88229112e-02, -3.84211391e-02],\n",
       "          [ 7.03224093e-02,  8.43758434e-02, -4.09858599e-02, ...,\n",
       "           -1.70417950e-02,  7.02109933e-02,  3.59029919e-02]], dtype=float32)),\n",
       "  'sentence-transformers/paraphrase-MiniLM-L6-v2': (array([[ 0.05844049,  0.04055947, -0.0149103 , ..., -0.048458  ,\n",
       "            0.01315738, -0.02953647],\n",
       "          [-0.1036999 , -0.05779901,  0.08626636, ..., -0.01957344,\n",
       "           -0.12219621, -0.02701517],\n",
       "          [-0.04226768, -0.09386408, -0.09615031, ..., -0.09826419,\n",
       "            0.11716352,  0.04875591],\n",
       "          ...,\n",
       "          [ 0.00566561,  0.0119043 ,  0.01756266, ..., -0.0908417 ,\n",
       "           -0.07077748,  0.06212011],\n",
       "          [ 0.02706615, -0.05268792, -0.04885602, ..., -0.02528192,\n",
       "           -0.04756318,  0.00221247],\n",
       "          [-0.09893444, -0.0223356 ,  0.03014206, ..., -0.09586029,\n",
       "            0.01353366, -0.02795785]], dtype=float32),\n",
       "   array([[ 6.64343685e-02,  7.49531686e-02,  6.72107637e-02, ...,\n",
       "           -2.09782682e-02,  7.66260922e-02, -3.10756695e-02],\n",
       "          [-7.49738738e-02, -1.08314365e-01, -2.35683261e-03, ...,\n",
       "           -8.17098171e-02, -6.27625063e-02,  3.25559899e-02],\n",
       "          [-6.91660047e-02, -1.64942350e-02, -5.99873401e-02, ...,\n",
       "            2.72994433e-02,  6.50795028e-02, -9.41403359e-02],\n",
       "          ...,\n",
       "          [-4.54027615e-02,  8.54803584e-05,  4.50570732e-02, ...,\n",
       "           -4.52784598e-02, -1.93260498e-02, -1.17575638e-02],\n",
       "          [ 3.49161364e-02, -1.02727398e-01,  3.88919190e-02, ...,\n",
       "           -1.28875583e-01,  2.59933267e-02,  2.74121650e-02],\n",
       "          [ 9.20626055e-03, -1.40455097e-01, -8.68771505e-03, ...,\n",
       "           -9.27553177e-02,  5.89218549e-02,  3.36104818e-02]], dtype=float32)),\n",
       "  'bert-base-uncased': (array([[ 0.05872061,  0.10280544, -0.37067536, ...,  0.02777471,\n",
       "           -0.30320382, -0.46605042],\n",
       "          [-0.00596239,  0.01722105, -0.23842913, ..., -0.0160418 ,\n",
       "           -0.2684856 , -0.47243172],\n",
       "          [ 0.00059247, -0.09853543, -0.20282477, ..., -0.07522403,\n",
       "           -0.48651347, -0.56841236],\n",
       "          ...,\n",
       "          [ 0.00606139,  0.09277672, -0.35181472, ..., -0.16836883,\n",
       "           -0.2937054 , -0.49755454],\n",
       "          [ 0.04491393,  0.02876331, -0.29186216, ..., -0.07845722,\n",
       "           -0.27190748, -0.5221855 ],\n",
       "          [ 0.07587137,  0.02218518, -0.2579178 , ..., -0.04948571,\n",
       "           -0.41170567, -0.5213806 ]], dtype=float32),\n",
       "   array([[ 0.04800436,  0.00203699, -0.17386767, ..., -0.02914538,\n",
       "           -0.3371133 , -0.53398293],\n",
       "          [-0.02776269,  0.13138437, -0.2491013 , ..., -0.02558256,\n",
       "           -0.2833394 , -0.5311247 ],\n",
       "          [-0.0522484 , -0.00995493, -0.31219012, ..., -0.08458441,\n",
       "           -0.25657678, -0.44415468],\n",
       "          ...,\n",
       "          [ 0.02897087,  0.03058032, -0.22355251, ..., -0.0557952 ,\n",
       "           -0.35955894, -0.57439446],\n",
       "          [ 0.07559036,  0.05485338, -0.32763126, ..., -0.06932916,\n",
       "           -0.27571774, -0.5227069 ],\n",
       "          [ 0.09702579,  0.02812839, -0.2464534 , ..., -0.06108326,\n",
       "           -0.41073206, -0.4938492 ]], dtype=float32)),\n",
       "  'roberta-base': (array([[ 3.1529507e-04,  1.7234702e-03,  9.8839486e-03, ...,\n",
       "            6.4861700e-03,  6.2437900e-03, -5.3695589e-04],\n",
       "          [ 3.7432702e-03,  1.4060249e-03,  1.3594428e-02, ...,\n",
       "            7.6605100e-03,  4.2032744e-03, -1.2195748e-03],\n",
       "          [ 4.1068764e-04,  8.5518407e-03,  6.2603601e-03, ...,\n",
       "            8.1076957e-03,  6.3982145e-03, -3.2363960e-03],\n",
       "          ...,\n",
       "          [-1.9526700e-03,  4.7987001e-03,  1.0353357e-02, ...,\n",
       "            7.0555522e-03,  7.3542790e-03, -5.6523946e-04],\n",
       "          [-2.5655343e-03,  4.2874352e-03,  1.2352586e-02, ...,\n",
       "            1.1720501e-02,  3.8491548e-03, -1.4670340e-03],\n",
       "          [-6.4736241e-03,  3.9010898e-03, -6.9624756e-04, ...,\n",
       "            8.4731570e-03,  1.6483735e-02,  8.8797009e-05]], dtype=float32),\n",
       "   array([[ 0.00063279,  0.00058343,  0.00668783, ...,  0.006902  ,\n",
       "            0.01060326,  0.00027376],\n",
       "          [ 0.00142826, -0.00081128,  0.01255272, ...,  0.00696046,\n",
       "            0.00855771, -0.00020021],\n",
       "          [-0.00126652,  0.00345354,  0.00820126, ...,  0.00755983,\n",
       "            0.00667504, -0.00102037],\n",
       "          ...,\n",
       "          [-0.00071792,  0.00449728,  0.00934791, ...,  0.00891141,\n",
       "            0.00521873, -0.00024201],\n",
       "          [ 0.00352595,  0.01230997,  0.01120177, ...,  0.01339264,\n",
       "            0.00041465, -0.00321827],\n",
       "          [ 0.00094994, -0.00175711,  0.01288581, ...,  0.01049003,\n",
       "            0.01080375,  0.00238274]], dtype=float32))},\n",
       " 'test': {'sentence-transformers/all-MiniLM-L6-v2': (array([[ 0.02073926, -0.01222432, -0.14897598, ..., -0.09109212,\n",
       "            0.09855671, -0.02626471],\n",
       "          [ 0.05298672, -0.21132165,  0.08660053, ..., -0.04350631,\n",
       "            0.186337  , -0.0797913 ],\n",
       "          [ 0.04750276, -0.0652224 ,  0.05706485, ..., -0.22859327,\n",
       "           -0.0958281 , -0.11036047],\n",
       "          ...,\n",
       "          [-0.06301006,  0.13458005,  0.23424302, ..., -0.09985761,\n",
       "            0.20896918, -0.15057409],\n",
       "          [ 0.06036918, -0.04850531, -0.13393384, ..., -0.086537  ,\n",
       "           -0.01772382, -0.05414686],\n",
       "          [-0.11807671, -0.06264173, -0.0380343 , ...,  0.01278111,\n",
       "            0.11220907,  0.07898127]], dtype=float32),\n",
       "   array([[-0.01259202,  0.01846165, -0.03918373, ..., -0.03455321,\n",
       "            0.10096515, -0.05108251],\n",
       "          [ 0.08373984, -0.04218713,  0.12761916, ...,  0.01430939,\n",
       "            0.22750261, -0.05664971],\n",
       "          [ 0.08667393,  0.07447664,  0.05410273, ..., -0.08356337,\n",
       "           -0.20732205, -0.08016814],\n",
       "          ...,\n",
       "          [-0.13778369,  0.11470126,  0.10635175, ..., -0.09227641,\n",
       "            0.186322  , -0.07409454],\n",
       "          [-0.03158988, -0.10870213, -0.00911232, ..., -0.08200142,\n",
       "           -0.08650208, -0.1475212 ],\n",
       "          [-0.1031615 , -0.03462666, -0.05987767, ..., -0.04083028,\n",
       "            0.10238579,  0.09468804]], dtype=float32)),\n",
       "  'sentence-transformers/paraphrase-MiniLM-L6-v2': (array([[-0.10249065, -0.05547256,  0.12244865, ..., -0.08477861,\n",
       "            0.08832833,  0.01730713],\n",
       "          [ 0.12373591,  0.03972934,  0.01383808, ..., -0.0650562 ,\n",
       "            0.01344042,  0.02443209],\n",
       "          [-0.06878904, -0.08483852, -0.09284716, ..., -0.05264251,\n",
       "            0.05384502,  0.03815033],\n",
       "          ...,\n",
       "          [-0.03280276, -0.02078171, -0.09339395, ..., -0.03582607,\n",
       "           -0.07694867,  0.09488319],\n",
       "          [-0.12054332,  0.08052579, -0.07039954, ...,  0.08887492,\n",
       "           -0.01717188,  0.0625543 ],\n",
       "          [ 0.17815475, -0.11153921, -0.11863554, ..., -0.00991233,\n",
       "            0.01106022,  0.07309458]], dtype=float32),\n",
       "   array([[-0.12625079, -0.10677596, -0.00922717, ..., -0.03295908,\n",
       "            0.07778637,  0.04651293],\n",
       "          [ 0.03992085,  0.0099544 , -0.08863437, ..., -0.0652297 ,\n",
       "            0.05473006,  0.03891141],\n",
       "          [ 0.01885016, -0.10658965, -0.02663776, ..., -0.08797231,\n",
       "           -0.00834932, -0.02861945],\n",
       "          ...,\n",
       "          [-0.02063805, -0.06481639, -0.10404929, ..., -0.05028567,\n",
       "           -0.05088703,  0.01379713],\n",
       "          [-0.15436256,  0.09828559, -0.12367645, ...,  0.08194251,\n",
       "           -0.09126325,  0.07858291],\n",
       "          [-0.09811691, -0.00957902, -0.06329051, ..., -0.2113232 ,\n",
       "           -0.07049286, -0.14843988]], dtype=float32)),\n",
       "  'bert-base-uncased': (array([[ 0.02867704, -0.01507326, -0.20083272, ..., -0.04798181,\n",
       "           -0.4529709 , -0.5770977 ],\n",
       "          [ 0.06817417,  0.06341024, -0.18817525, ..., -0.0087606 ,\n",
       "           -0.33536807, -0.5344676 ],\n",
       "          [ 0.10694245,  0.01993571, -0.28453222, ..., -0.0998642 ,\n",
       "           -0.32334122, -0.4770769 ],\n",
       "          ...,\n",
       "          [ 0.05904689,  0.06438994, -0.380385  , ..., -0.01396599,\n",
       "           -0.26695144, -0.5198718 ],\n",
       "          [ 0.08786552,  0.02247874, -0.27463073, ..., -0.10930587,\n",
       "           -0.32168365, -0.6023234 ],\n",
       "          [ 0.0348909 , -0.02307181, -0.20235373, ...,  0.00449436,\n",
       "           -0.2829199 , -0.53979385]], dtype=float32),\n",
       "   array([[ 0.03523577, -0.0437851 , -0.22006883, ..., -0.07901935,\n",
       "           -0.41877934, -0.5271362 ],\n",
       "          [ 0.08049241,  0.08873083, -0.2520839 , ...,  0.00922653,\n",
       "           -0.34477812, -0.50328153],\n",
       "          [ 0.15371105,  0.01883647, -0.29250202, ..., -0.09180999,\n",
       "           -0.31413996, -0.46564478],\n",
       "          ...,\n",
       "          [ 0.09652419,  0.11729676, -0.26016963, ..., -0.05141638,\n",
       "           -0.40024385, -0.48508915],\n",
       "          [ 0.05795908,  0.05668132, -0.28588268, ..., -0.02313231,\n",
       "           -0.25092864, -0.4708504 ],\n",
       "          [ 0.08382282,  0.00291444, -0.2503442 , ..., -0.03377965,\n",
       "           -0.397035  , -0.5536575 ]], dtype=float32)),\n",
       "  'roberta-base': (array([[-6.7478074e-03, -5.6850666e-05,  1.2701863e-02, ...,\n",
       "            1.0909729e-02,  7.5845872e-03, -9.7624604e-03],\n",
       "          [-6.0343565e-03,  2.3649579e-03,  1.3237429e-02, ...,\n",
       "            1.1253519e-03,  6.0593062e-03, -4.8091286e-04],\n",
       "          [-2.3435929e-03,  7.5918552e-03,  1.8141247e-02, ...,\n",
       "           -9.3134929e-04,  8.6158952e-03, -2.5417684e-03],\n",
       "          ...,\n",
       "          [ 1.2175586e-03, -5.1344787e-03,  9.8095164e-03, ...,\n",
       "            9.5714871e-03,  1.2788201e-02, -3.4545390e-03],\n",
       "          [ 1.1173417e-03,  2.2484856e-03,  1.3051089e-02, ...,\n",
       "           -7.3221326e-04,  1.0118183e-02, -3.4427973e-03],\n",
       "          [-1.5281455e-03, -2.2044808e-03,  1.7293110e-02, ...,\n",
       "            1.0920187e-02,  8.5598277e-03, -3.0665789e-03]], dtype=float32),\n",
       "   array([[-5.4103592e-03,  1.0587686e-02,  8.5082529e-03, ...,\n",
       "            1.1483428e-02, -1.7796876e-04, -1.3498778e-02],\n",
       "          [ 3.7694389e-03, -1.7404071e-03,  1.2156236e-02, ...,\n",
       "            8.0375569e-03,  5.7533155e-03, -4.9700555e-03],\n",
       "          [-1.4003159e-03,  5.9915851e-03,  1.5709756e-02, ...,\n",
       "            3.3696089e-03,  1.1158774e-02,  8.0776424e-04],\n",
       "          ...,\n",
       "          [ 8.4165181e-04, -5.7212012e-03,  7.9477355e-03, ...,\n",
       "            8.4817782e-03,  1.0687920e-02, -2.2590344e-03],\n",
       "          [ 9.3145552e-04,  1.6466414e-03,  1.4707230e-02, ...,\n",
       "            1.8842466e-03,  4.9977689e-03,  1.4732126e-05],\n",
       "          [-4.6556955e-03,  1.0518326e-03,  7.7202339e-03, ...,\n",
       "            9.4303619e-03,  3.0711922e-03, -5.6330152e-03]], dtype=float32))},\n",
       " 'validation': {'sentence-transformers/all-MiniLM-L6-v2': (array([[-0.0136769 , -0.24111955, -0.07200541, ..., -0.02488795,\n",
       "           -0.1636071 , -0.11929958],\n",
       "          [ 0.01057296, -0.19666392,  0.07833286, ..., -0.03142624,\n",
       "            0.13138162,  0.01478035],\n",
       "          [ 0.00265693,  0.04696179, -0.03080882, ..., -0.1191259 ,\n",
       "            0.00333901, -0.08585491],\n",
       "          ...,\n",
       "          [-0.08955676, -0.00134045,  0.01924863, ...,  0.04782216,\n",
       "            0.07609054, -0.05854713],\n",
       "          [-0.16951445,  0.05631033,  0.08635007, ..., -0.10816055,\n",
       "           -0.06368531, -0.03756031],\n",
       "          [ 0.11805306,  0.05849817, -0.0328052 , ...,  0.00471786,\n",
       "           -0.04932877,  0.01076657]], dtype=float32),\n",
       "   array([[-0.00777205, -0.23985696, -0.02208045, ..., -0.02632052,\n",
       "           -0.13766882, -0.06401653],\n",
       "          [ 0.07001863, -0.19170588,  0.11092471, ..., -0.03396466,\n",
       "            0.11672941, -0.02472459],\n",
       "          [-0.08018854, -0.03840542, -0.10261239, ..., -0.21734712,\n",
       "            0.02613015, -0.06338237],\n",
       "          ...,\n",
       "          [-0.16038746, -0.00460138,  0.0138003 , ..., -0.22164899,\n",
       "           -0.16731839, -0.08028428],\n",
       "          [-0.13842155, -0.04442645,  0.10388105, ..., -0.11833847,\n",
       "           -0.11470255, -0.07757246],\n",
       "          [ 0.11483474,  0.06715774, -0.0533023 , ..., -0.06793462,\n",
       "            0.06038994, -0.05273217]], dtype=float32)),\n",
       "  'sentence-transformers/paraphrase-MiniLM-L6-v2': (array([[-0.17943816, -0.15285976, -0.0099992 , ..., -0.0266095 ,\n",
       "           -0.08666682, -0.05968959],\n",
       "          [ 0.06899769, -0.08716084,  0.04946247, ..., -0.06456396,\n",
       "           -0.01561866, -0.02539325],\n",
       "          [ 0.05386107,  0.02039447, -0.00020403, ..., -0.08400811,\n",
       "           -0.06829314, -0.10002272],\n",
       "          ...,\n",
       "          [-0.00920583,  0.00685183, -0.07628044, ..., -0.01850929,\n",
       "           -0.00415255, -0.07891245],\n",
       "          [-0.18665087, -0.02161497,  0.02287774, ..., -0.05896826,\n",
       "           -0.04030266,  0.02676994],\n",
       "          [ 0.07155665, -0.10282605, -0.07308531, ..., -0.16833471,\n",
       "            0.06303033,  0.1494466 ]], dtype=float32),\n",
       "   array([[-0.03445182, -0.12226163, -0.11648571, ..., -0.06227656,\n",
       "            0.1368129 , -0.06892832],\n",
       "          [-0.01223935,  0.00612019,  0.05191489, ..., -0.07287344,\n",
       "           -0.00891338,  0.02736447],\n",
       "          [ 0.03893321,  0.02073304, -0.01902978, ..., -0.07841101,\n",
       "           -0.06758574, -0.09516368],\n",
       "          ...,\n",
       "          [ 0.02259078,  0.01781222, -0.09422186, ..., -0.02678574,\n",
       "            0.04066673,  0.00623182],\n",
       "          [-0.17498846, -0.03814461, -0.01432672, ..., -0.01363384,\n",
       "           -0.05799656,  0.07919244],\n",
       "          [ 0.09681304, -0.09476862, -0.10110287, ..., -0.09624805,\n",
       "           -0.06673238, -0.00469037]], dtype=float32)),\n",
       "  'bert-base-uncased': (array([[ 0.00307266, -0.00946232, -0.20261322, ..., -0.00833224,\n",
       "           -0.36705923, -0.5913451 ],\n",
       "          [ 0.123526  ,  0.02077827, -0.3056492 , ...,  0.02955711,\n",
       "           -0.3225697 , -0.48150814],\n",
       "          [ 0.02983784,  0.03604545, -0.29000968, ..., -0.01160391,\n",
       "           -0.23297715, -0.46530896],\n",
       "          ...,\n",
       "          [ 0.03323108,  0.04961846, -0.19192022, ..., -0.01441159,\n",
       "           -0.31824154, -0.52096564],\n",
       "          [ 0.08581503,  0.02150352, -0.2083014 , ..., -0.04567692,\n",
       "           -0.352328  , -0.5713405 ],\n",
       "          [ 0.09613273,  0.01911142, -0.3481592 , ..., -0.02187737,\n",
       "           -0.28752452, -0.5093987 ]], dtype=float32),\n",
       "   array([[ 0.04191368,  0.03538987, -0.1839706 , ..., -0.04024925,\n",
       "           -0.41864008, -0.54072267],\n",
       "          [ 0.05670279,  0.03521055, -0.2530106 , ...,  0.04181685,\n",
       "           -0.33573115, -0.48871902],\n",
       "          [ 0.0299292 ,  0.04991373, -0.2957147 , ...,  0.00210789,\n",
       "           -0.21472012, -0.45475787],\n",
       "          ...,\n",
       "          [ 0.09765518,  0.03427759, -0.26722658, ..., -0.03337018,\n",
       "           -0.31906074, -0.5119372 ],\n",
       "          [ 0.11352745,  0.0099492 , -0.27010715, ..., -0.06129587,\n",
       "           -0.39899376, -0.5444081 ],\n",
       "          [ 0.04410383,  0.02098046, -0.38280034, ..., -0.08397419,\n",
       "           -0.29294217, -0.43081176]], dtype=float32)),\n",
       "  'roberta-base': (array([[ 0.00184951,  0.0056218 ,  0.00956417, ...,  0.01514118,\n",
       "            0.00214623,  0.00224721],\n",
       "          [-0.00285759,  0.00126444,  0.01028903, ...,  0.01360386,\n",
       "            0.0161304 , -0.00265204],\n",
       "          [ 0.00657542, -0.00197218,  0.01429076, ...,  0.00663549,\n",
       "            0.00093848, -0.00662739],\n",
       "          ...,\n",
       "          [-0.00588467, -0.00321537,  0.01059124, ..., -0.00112211,\n",
       "            0.00531791,  0.00095934],\n",
       "          [ 0.00909607,  0.0008464 ,  0.00687786, ...,  0.01082157,\n",
       "            0.00390289, -0.00223263],\n",
       "          [ 0.00225439,  0.00792104,  0.00630825, ...,  0.00660387,\n",
       "            0.00488609, -0.00399229]], dtype=float32),\n",
       "   array([[ 0.00317735, -0.00107134,  0.01408124, ...,  0.0161485 ,\n",
       "            0.0011348 , -0.00280499],\n",
       "          [-0.00182869,  0.00846638,  0.01518089, ...,  0.0120217 ,\n",
       "            0.01014493, -0.00058162],\n",
       "          [ 0.00020989, -0.00513476,  0.00358572, ...,  0.00569145,\n",
       "            0.00339824, -0.00122853],\n",
       "          ...,\n",
       "          [-0.00442885, -0.00162382,  0.01246709, ..., -0.00632305,\n",
       "            0.01073407,  0.00189959],\n",
       "          [ 0.0073089 , -0.00563   ,  0.0075    , ...,  0.01278808,\n",
       "            0.00914025, -0.00497255],\n",
       "          [ 0.00485108,  0.00077116,  0.00892446, ...,  0.00764454,\n",
       "            0.00609341, -0.00610533]], dtype=float32))}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm9N1jgcKwHq"
   },
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_L1Db1CVTdh"
   },
   "source": [
    " ### using Base ensmble model as Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "PiiP90jJekaH"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def train_and_evaluate_from_embeddings(embedding_data, label_data):\n",
    "    for model_name in embedding_data['train']:\n",
    "        print(f\"\\n--- Evaluating model: {model_name} ---\")\n",
    "\n",
    "        # Get embeddings\n",
    "        q1_train, q2_train = embedding_data['train'][model_name]\n",
    "        q1_test, q2_test = embedding_data['test'][model_name]\n",
    "\n",
    "        # Combine embeddings (concatenate q1 and q2)\n",
    "        X_train = np.concatenate([q1_train, q2_train], axis=1)\n",
    "        X_test = np.concatenate([q1_test, q2_test], axis=1)\n",
    "\n",
    "        # Get labels for this split\n",
    "        y_train = label_data['train']\n",
    "        y_test = label_data['test']\n",
    "\n",
    "        # Safety check in case label and embedding lengths don't match\n",
    "        if len(X_train) != len(y_train) or len(X_test) != len(y_test):\n",
    "            print(f\"Mismatch in data and label sizes for model {model_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Train classifier\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfiHzn4jSBoK"
   },
   "source": [
    "#### compute similarity instead of using embeddings directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "s0KBjpL6L8YW"
   },
   "outputs": [],
   "source": [
    "# --- More complex similarity features ---\n",
    "def compute_advanced_similarity_features(q1_emb, q2_emb):\n",
    "    features = []\n",
    "    for q1, q2 in zip(q1_emb, q2_emb):\n",
    "        cos_sim = cosine_similarity(q1.reshape(1, -1), q2.reshape(1, -1))[0][0]\n",
    "        euc_dist = euclidean_distances(q1.reshape(1, -1), q2.reshape(1, -1))[0][0]\n",
    "        dot_prod = np.dot(q1, q2)\n",
    "        l1_dist = np.sum(np.abs(q1 - q2))\n",
    "        abs_diff = np.abs(q1 - q2)\n",
    "        sq_diff = (q1 - q2) ** 2\n",
    "\n",
    "        combined = np.concatenate((\n",
    "            [cos_sim, euc_dist, dot_prod, l1_dist],\n",
    "            abs_diff,\n",
    "            sq_diff\n",
    "        ))\n",
    "        features.append(combined)\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "JNOJlrMIL8bl"
   },
   "outputs": [],
   "source": [
    "# --- Build feature set from all models (train split) ---\n",
    "feature_list = []\n",
    "for model_name in models:\n",
    "    q1_emb, q2_emb = embedding_data['train'][model_name]\n",
    "    features = compute_advanced_similarity_features(q1_emb, q2_emb)\n",
    "    feature_list.append(features)\n",
    "\n",
    "# Combine features from all models\n",
    "X = np.hstack(feature_list)\n",
    "\n",
    "# Use the pre-sampled aligned labels\n",
    "y = label_data['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "cLG36W_WL8fZ",
    "outputId": "484d01d2-e8c5-44bf-bc40-5abcc88f1486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       120\n",
      "           1       0.76      0.69      0.72        80\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.78      0.77      0.78       200\n",
      "weighted avg       0.79      0.79      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split into train/test subsets from constructed X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "CYfsqYsgMDsO"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open(\"ensemble_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKfA19UnSIvR"
   },
   "source": [
    "#### Use embeddings directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zo1P697ESI5q",
    "outputId": "cbf0e5be-8c45-49a9-bcbd-c2c2e554cbbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.97      0.75       120\n",
      "           1       0.57      0.05      0.09        80\n",
      "\n",
      "    accuracy                           0.60       200\n",
      "   macro avg       0.59      0.51      0.42       200\n",
      "weighted avg       0.59      0.60      0.49       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# --- Combine raw embeddings from all models (concatenation) ---\n",
    "full_embeddings = []\n",
    "for model_name in models:\n",
    "    q1_emb, q2_emb = embedding_data['train'][model_name]  \n",
    "    combined = np.hstack((q1_emb, q2_emb))  # Direct concatenation\n",
    "    full_embeddings.append(combined)\n",
    "\n",
    "# Stack all model embeddings into one feature vector\n",
    "X = np.hstack(full_embeddings)\n",
    "\n",
    "# Use aligned labels\n",
    "y = label_data['train']\n",
    "\n",
    "# --- Train & evaluate ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVpfIHiCJWKP",
    "outputId": "90549d18-c350-43e6-deb9-0125e23b3727"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EsMygqFTBnL"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open(\"ensemble_model_concat.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wBRVm1vTK5U"
   },
   "source": [
    "#### using CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbhFXGn6TLRM",
    "outputId": "bd83aae1-d1d0-4e34-d538-1c18458ed184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.99      0.75       120\n",
      "           1       0.00      0.00      0.00        80\n",
      "\n",
      "    accuracy                           0.59       200\n",
      "   macro avg       0.30      0.50      0.37       200\n",
      "weighted avg       0.36      0.59      0.45       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# --- Apply CCA to reduce redundancy between q1 and q2 embeddings across all models ---\n",
    "full_embeddings_q1 = []\n",
    "full_embeddings_q2 = []\n",
    "\n",
    "for model_name in models:\n",
    "    q1_emb, q2_emb = embedding_data['train'][model_name]  #  Access correct split\n",
    "    full_embeddings_q1.append(q1_emb)\n",
    "    full_embeddings_q2.append(q2_emb)\n",
    "\n",
    "X_q1 = np.hstack(full_embeddings_q1)\n",
    "X_q2 = np.hstack(full_embeddings_q2)\n",
    "\n",
    "# Determine safe upper bound for CCA components\n",
    "max_components = min(X_q1.shape[0], X_q1.shape[1], X_q2.shape[1])\n",
    "n_components = min(200, max_components)  # You can adjust the target value here\n",
    "cca = CCA(n_components=n_components, max_iter=1000)  # or higher if needed\n",
    "X_q1_cca, X_q2_cca = cca.fit_transform(X_q1, X_q2)\n",
    "\n",
    "\n",
    "# Final feature vector\n",
    "X = np.hstack((X_q1_cca, X_q2_cca))\n",
    "\n",
    "# Use pre-sampled, aligned labels\n",
    "y = label_data['train']\n",
    "\n",
    "# --- Train & evaluate model ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model to file\n",
    "with open(\"ensemble_model_CCA.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)\n",
    "    \n",
    "with open(\"cca_transform.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cca, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8rdaQM_KZHl",
    "outputId": "6edaa73d-ef94-4bad-c2e8-16dca3763fb8"
   },
   "outputs": [],
   "source": [
    "X_q1_cca.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LVR32zQUPgY"
   },
   "outputs": [],
   "source": [
    "# Save the transformed embeddings as NumPy files\n",
    "np.save(\"X_q1_cca.npy\", X_q1_cca)\n",
    "np.save(\"X_q2_cca.npy\", X_q2_cca)\n",
    "np.save(\"X_cca.npy\", X)  # Final combined feature vector\n",
    "np.save(\"y_labels.npy\", y)  # Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTUPWp-jZepn"
   },
   "source": [
    "### using Base ensmble model as Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTSjz584ZoLA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MA97Ep_YK0kz"
   },
   "source": [
    "# Model Testing"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
